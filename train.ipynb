{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import our model\n",
    "from model import MVP\n",
    "\n",
    "# Libraries for the data\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# Get the sector tickers\n",
    "from sector import *\n",
    "\n",
    "# Process the data\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "tks = get_ticker_dict(sectors)\n",
    "descs = get_desc_dict(sectors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get our data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "sectors_data = {}\n",
    "for k in tks.keys():\n",
    "    sectors_data[k] = pd.read_csv('data/sectors/TA/{}_7yr_daily.csv'.format(k))\n",
    "    sectors_data[k].index = pd.to_datetime(sectors_data[k][\"date\"])\n",
    "    sectors_data[k].drop(['ticker', 'descr', 'date'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "sp5 = pd.read_csv('data/sectors/SP500_7yr_daily.csv')\n",
    "sp5.index = pd.to_datetime(sp5[\"date\"])\n",
    "sp5.drop(['ticker', 'descr', 'date'], axis=1, inplace=True)\n",
    "labels = pd.read_csv('data/sectors/sector_labels.csv')\n",
    "labels.index = pd.to_datetime(labels[\"date\"])\n",
    "labels.drop('date', axis=1, inplace=True)\n",
    "labels.drop('Unnamed: 0', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "ordering = []\n",
    "for k in sectors_data.keys():\n",
    "    ordering += [k]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ERROR: No XLP data for date 2015-06-01T00:00:00.000000000 where expected. Using previous day of 2015-05-29 00:00:00.\n",
      "2015-05-29 00:00:00\n",
      "ERROR: No XLP data for date 2015-10-01T00:00:00.000000000 where expected. Using previous day of 2015-09-30 00:00:00.\n",
      "2015-09-30 00:00:00\n",
      "ERROR: No XLRE data for date 2017-09-01T00:00:00.000000000 where expected. Using previous day of 2017-08-31 00:00:00.\n",
      "2017-08-31 00:00:00\n",
      "ERROR: No XLB data for date 2017-09-29T00:00:00.000000000 where expected. Using previous day of 2017-09-28 00:00:00.\n",
      "2017-09-28 00:00:00\n",
      "ERROR: No XLP data for date 2018-06-01T00:00:00.000000000 where expected. Using previous day of 2018-05-31 00:00:00.\n",
      "2018-05-31 00:00:00\n",
      "ERROR: No XLF data for date 2018-06-29T00:00:00.000000000 where expected. Using previous day of 2018-06-28 00:00:00.\n",
      "2018-06-28 00:00:00\n",
      "ERROR: No XLF data for date 2018-08-01T00:00:00.000000000 where expected. Using previous day of 2018-07-31 00:00:00.\n",
      "2018-07-31 00:00:00\n",
      "ERROR: No XLRE data for date 2019-05-01T00:00:00.000000000 where expected. Using previous day of 2019-04-30 00:00:00.\n",
      "2019-04-30 00:00:00\n",
      "ERROR: No XLE data for date 2019-08-30T00:00:00.000000000 where expected. Using previous day of 2019-08-29 00:00:00.\n",
      "2019-08-29 00:00:00\n",
      "ERROR: No XLU data for date 2021-10-01T00:00:00.000000000 where expected. Using previous day of 2021-09-30 00:00:00.\n",
      "2021-09-30 00:00:00\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "converts a dataframe to be valid input data and labels by lining up dates and adding a row of data for each sector (and the S&P)\n",
    "'''\n",
    "def data_to_XY(sectors, sp5, labels):\n",
    "    closest_date = None\n",
    "    \n",
    "    # 1 entry for every month date\n",
    "    stacks = len(labels)\n",
    "    # Number of stocks (sectors & the SP500)\n",
    "    stack_height = len(sectors.keys()) + 1 \n",
    "    # 1 col for every feature (subtract ticker column and description column)\n",
    "    stack_width = len(sectors[\"XLB\"].columns)\n",
    "    \n",
    "    X = torch.zeros((stacks, stack_height, stack_width), dtype=torch.float64)\n",
    "    \n",
    "    # Go through dates and gather rows of each sector & sp5\n",
    "    for i, date in enumerate(labels.index):\n",
    "        newdf = sp5.index[sp5.index <= date]\n",
    "\n",
    "        ### Add SP5 data to row\n",
    "        # Get the last date before the new month\n",
    "        closest_date = newdf.values[-1]\n",
    "        \n",
    "        # Do i need to deep copy here?\n",
    "        row = torch.zeros((stack_width))\n",
    "        row[0:2] = torch.tensor(sp5.iloc[sp5.index == closest_date].values[0])\n",
    "        X[i,0] = row\n",
    "        \n",
    "        row_counter = 0\n",
    "        ### Add sectors' data to row\n",
    "        for k in sectors.keys():\n",
    "            sector = sectors[k]\n",
    "            row_counter += 1\n",
    "            sector_row = torch.zeros((stack_width))\n",
    "            \n",
    "            # Already have the last date, make sure the sectors align with that date (have a row of information for each date)\n",
    "            # If the sector started at a date beyond our last date, ignore it (all 0s)\n",
    "            if sector.index.values[0] > closest_date:\n",
    "                # print(\"No {} data at/before date: {}... settings to 0s\".format(k, closest_date))\n",
    "                X[i,row_counter] = sector_row\n",
    "            else:\n",
    "                \n",
    "                # Should expect to have data here. If not, this is a problem (has SP5 data and previous sector data, but this would imply a hole in the data)\n",
    "                if len(sector.iloc[sector.index == closest_date]) != 1:\n",
    "                    print(\"ERROR: No {} data for date {} where expected. Using previous day of {}.\".format(k, closest_date, sector.iloc[sector.index < closest_date].index[-1]))\n",
    "                    sector_row = torch.tensor(sector.iloc[sector.index < closest_date].values[-1,:])\n",
    "                else:\n",
    "                    sector_row = torch.tensor(sector.iloc[sector.index == closest_date].values[0])\n",
    "                \n",
    "                X[i,row_counter] = sector_row\n",
    "            \n",
    "    # Prepare the labels\n",
    "    Y = torch.zeros((labels.shape[0], labels.shape[1]), dtype=torch.float64)\n",
    "    for i, col in enumerate(labels.columns):\n",
    "        index = ordering.index(descs[col])\n",
    "        Y[:,index] = torch.tensor(labels[col].values)\n",
    "    \n",
    "    return X, Y\n",
    "    \n",
    "X, Y = data_to_XY(sectors_data, sp5, labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Examine Data\n",
    "Our X data consists of 3 dimensions: (monthly data points, each sector of the S&P500 and the S&P500 itself, TA features for each stock)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(82, 12, 94)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our Y data consists of 2 dimensions (monthly data points, 1 value for each of the sectors). Note that the sector dimension is 1 less than the X data (no labels for S&P500 data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([82, 11])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train/Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.33, random_state=226)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scale our data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "sclr = StandardScaler()\n",
    "num_instances, num_stocks, num_features = X_train.shape\n",
    "X_train = X_train.reshape((-1, num_features))\n",
    "X_train = sclr.fit_transform(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Return X_train data to how it was\n",
    "X_train = torch.from_numpy(X_train.reshape((num_instances, num_stocks, num_features)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This will be for testing\n",
    "num_instances, num_stocks, num_features = X_test.shape\n",
    "X_test = X_test.reshape((-1, num_features))\n",
    "X_test = sclr.transform(X_test)\n",
    "X_test = torch.from_numpy(X_test.reshape((num_instances, num_stocks, num_features)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset and Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "class StockDataset(torch.utils.data.Dataset):\n",
    "  def __init__(self, X, Y):\n",
    "    self.X = X\n",
    "    self.Y = Y\n",
    "\n",
    "  def __len__(self):\n",
    "    return len(self.X)\n",
    "\n",
    "  def __getitem__(self, index):\n",
    "    return self.X[index], self.Y[index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = StockDataset(X_train, y_train)\n",
    "test_dataset = StockDataset(X_test, y_test)\n",
    "params = {'batch_size': 1,\n",
    "          'shuffle': True,\n",
    "          'num_workers': 0}\n",
    "train_dataloader = torch.utils.data.DataLoader(train_dataset, **params)\n",
    "test_dataloader = torch.utils.data.DataLoader(test_dataset, **params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input Dimension is the number of features * number of stocks (because it will be flattened),\n",
    "# Output Dimension is the number of stocks - 1 (because only predicting if each stock will over/underperform the S&P500, -1 because S&P500 included in data)\n",
    "# Output Dimension is 1, indicating we want 1 value to determine if each stock will over/underperform the S&P500\n",
    "model = MVP(X.shape[1] * X.shape[2], Y.shape[1])\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, optimizer, epochs=50, ret_loss=False):\n",
    "\n",
    "  criterion = nn.CrossEntropyLoss()\n",
    "  loss_data = torch.zeros((epochs, len(train_dataloader)))\n",
    "  for e in range(epochs):\n",
    "\n",
    "    run_loss, ct = 0, 0\n",
    "    batch_loss = 0.0\n",
    "    for i, data in enumerate(train_dataloader, 0):\n",
    "      X, Y = data\n",
    "      X = X.float()\n",
    "      optimizer.zero_grad()\n",
    "\n",
    "      pred = model(X)\n",
    "      loss = criterion(pred.reshape(-1), Y.reshape(-1))\n",
    "      loss.backward()\n",
    "      optimizer.step()\n",
    "\n",
    "      # print statistics\n",
    "      l = loss.item()\n",
    "      loss_data[e,i] = l\n",
    "      batch_loss += l\n",
    "      run_loss += l\n",
    "      ct += 1\n",
    "      if i % 10 == 9:    # print every 10 mini-batches\n",
    "          print(f'[{e + 1}, {i + 1:5d}] loss: {batch_loss / 2000:.3f}')\n",
    "          batch_loss = 0.0\n",
    "    print(f\"Epoch loss: {run_loss / len(train_dataloader):.3f}\")\n",
    "  print('Finished Training')\n",
    "  if ret_loss:\n",
    "    return loss_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "Dimension out of range (expected to be in range of [-1, 0], but got 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-123-fdb63027ecc2>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-122-fbcf212c3447>\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(model, optimizer, epochs, ret_loss)\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m       \u001b[0mpred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 15\u001b[1;33m       \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpred\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mY\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     16\u001b[0m       \u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m       \u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    726\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 727\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[0;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\torch\\nn\\modules\\loss.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input, target)\u001b[0m\n\u001b[0;32m    959\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    960\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 961\u001b[1;33m         return F.cross_entropy(input, target, weight=self.weight,\n\u001b[0m\u001b[0;32m    962\u001b[0m                                ignore_index=self.ignore_index, reduction=self.reduction)\n\u001b[0;32m    963\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\torch\\nn\\functional.py\u001b[0m in \u001b[0;36mcross_entropy\u001b[1;34m(input, target, weight, size_average, ignore_index, reduce, reduction)\u001b[0m\n\u001b[0;32m   2466\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0msize_average\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mreduce\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2467\u001b[0m         \u001b[0mreduction\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_Reduction\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlegacy_get_string\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msize_average\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreduce\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2468\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mnll_loss\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlog_softmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mignore_index\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreduction\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2469\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2470\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\torch\\nn\\functional.py\u001b[0m in \u001b[0;36mlog_softmax\u001b[1;34m(input, dim, _stacklevel, dtype)\u001b[0m\n\u001b[0;32m   1603\u001b[0m         \u001b[0mdim\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_get_softmax_dim\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'log_softmax'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdim\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_stacklevel\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1604\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mdtype\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1605\u001b[1;33m         \u001b[0mret\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlog_softmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdim\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1606\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1607\u001b[0m         \u001b[0mret\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlog_softmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdim\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mIndexError\u001b[0m: Dimension out of range (expected to be in range of [-1, 0], but got 1)"
     ]
    }
   ],
   "source": [
    "train(model, optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "out = model(X_train[0].float())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "fd9765cc4a12c8f620395c2183698124ccce9dfda40da83c1115d13f985f26dc"
  },
  "kernelspec": {
   "display_name": "Python 3.8.6 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
